{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55a74dac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cerveau branché sur : cuda\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gymnasium as gym\n",
    "import gym_trading_env\n",
    "from gym_trading_env.wrapper import DiscreteActionsWrapper\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import wandb\n",
    "\n",
    "# Vérification si on peut utiliser la carte graphique (GPU) ou juste le processeur (CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Cerveau branché sur : {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e806ce93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- CELLULE 2 OPTIMISÉE (Mode Turbo) ---\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        \n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = 0.95\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_min = 0.01\n",
    "        self.epsilon_decay = 0.9995\n",
    "        self.learning_rate = 0.001\n",
    "        \n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(state_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, action_size)\n",
    "        ).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        self.criterion = nn.MSELoss()\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        \n",
    "        state = torch.FloatTensor(state).to(device)\n",
    "        with torch.no_grad():\n",
    "            act_values = self.model(state)\n",
    "        return torch.argmax(act_values).item()\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        # --- OPTIMISATION VECTORIELLE (Le Turbo) ---\n",
    "        # Au lieu d'une boucle for, on transforme tout le batch en gros tenseurs d'un coup\n",
    "        \n",
    "        # On empile les états (Batch, 1, 5) -> (Batch, 5)\n",
    "        states = torch.FloatTensor(np.array([t[0] for t in minibatch])).squeeze(1).to(device)\n",
    "        actions = torch.LongTensor(np.array([t[1] for t in minibatch])).unsqueeze(1).to(device)\n",
    "        rewards = torch.FloatTensor(np.array([t[2] for t in minibatch])).to(device)\n",
    "        next_states = torch.FloatTensor(np.array([t[3] for t in minibatch])).squeeze(1).to(device)\n",
    "        dones = torch.FloatTensor(np.array([t[4] for t in minibatch])).to(device)\n",
    "\n",
    "        # 1. Prédiction actuelle : Q(s, a)\n",
    "        # On demande au réseau de calculer pour les 32 états d'un coup\n",
    "        # .gather(1, actions) permet de ne garder que la valeur de l'action qui a été vraiment jouée\n",
    "        current_q = self.model(states).gather(1, actions).squeeze(1)\n",
    "\n",
    "        # 2. Prédiction future : max Q(s', a')\n",
    "        # On calcule le max des actions futures possibles\n",
    "        next_q = self.model(next_states).max(1)[0].detach()\n",
    "\n",
    "        # 3. Calcul de la cible (Target)\n",
    "        target_q = rewards + (self.gamma * next_q * (1 - dones))\n",
    "\n",
    "        # 4. Apprentissage (Une seule passe pour tout le monde !)\n",
    "        loss = self.criterion(current_q, target_q)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba161791",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data pipeline: 9 fichiers détectés prêts pour l'environnement.\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "\n",
    "def preprocess(df):\n",
    "    \"\"\"\n",
    "    Fonction de prétraitement des données financières.\n",
    "    Objectif : Rendre les données stationnaires pour le réseau de neurones.\n",
    "    \"\"\"\n",
    "    # 1) Nettoyage structurel\n",
    "    df = df.sort_index().dropna().drop_duplicates()\n",
    "\n",
    "    # 2) Feature Engineering\n",
    "    # Utilisation des rendements logarithmiques (log returns) préférables aux % bruts pour les réseaux de neurones\n",
    "    # car ils sont symétriques et additifs.\n",
    "    df[\"feature_close\"] = np.log(df[\"close\"]).diff()\n",
    "    df[\"feature_high\"] = np.log(df[\"high\"]).diff()\n",
    "    df[\"feature_low\"] = np.log(df[\"low\"]).diff()\n",
    "\n",
    "    # on ignore le volume pour l'instant pour éviter les divisions par zéro sur le Forex/CFD\n",
    "    # Si nécessaire plus tard, ajouter une gestion d'erreur spécifique (fillna ou epsilon).\n",
    "    \n",
    "    # 3) Nettoyage final (suppression des NaN générés par diff())\n",
    "    df = df.dropna()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Récupération de la liste des fichiers de données\n",
    "# Assurez-vous que le dossier 'data' est à la racine du notebook\n",
    "dataset_path = \"./data/*.pkl\" \n",
    "files = glob.glob(dataset_path)\n",
    "\n",
    "if not files:\n",
    "    print(f\"WARNING: Aucun fichier trouvé dans {dataset_path}. Vérifiez le chemin.\")\n",
    "else:\n",
    "    print(f\"Data pipeline: {len(files)} fichiers détectés prêts pour l'environnement.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc787ecd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chargement de l'environnement depuis : ./data/*.pkl\n",
      "Environnement initialisé.\n",
      "Inputs (Ce que voit le robot) : (5,)\n",
      "Outputs (Boutons disponibles) : 3\n"
     ]
    }
   ],
   "source": [
    "# Définition des métriques personnalisées pour le suivi\n",
    "def metric_portfolio_valuation(history):\n",
    "    return history['portfolio_valuation', -1]\n",
    "\n",
    "# On définit le chemin sous forme de texte (String) avec une étoile pour dire \"tous les pkl\"\n",
    "# C'est ce format que Windows et la librairie préfèrent.\n",
    "dataset_path_str = \"./data/*.pkl\"\n",
    "\n",
    "print(f\"Chargement de l'environnement depuis : {dataset_path_str}\")\n",
    "\n",
    "# Création de l'environnement Gym\n",
    "env_raw = gym.make(\n",
    "    \"MultiDatasetTradingEnv\",\n",
    "    dataset_dir=dataset_path_str,     # <--- CORRECTION ICI : On donne le string \"*.pkl\"\n",
    "    preprocess=preprocess,            # Ta fonction définie dans la cellule 3\n",
    "    portfolio_initial_value=1_000,    # Consigne: 1000$ initial\n",
    "    trading_fees=0.1/100,             # Consigne: 0.1% frais\n",
    "    borrow_interest_rate=0.02/100/24, # Consigne: 0.02% jour\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Ajout de la métrique de valuation pour WandB\n",
    "env_raw.add_metric('Valuation Finale', metric_portfolio_valuation)\n",
    "\n",
    "# Wrapper pour actions discrètes\n",
    "# Actions : 0 = Short (-1), 1 = Neutral (0), 2 = Long (1)\n",
    "env = DiscreteActionsWrapper(env_raw, positions=[-1, 0, 1])\n",
    "\n",
    "print(f\"Environnement initialisé.\")\n",
    "print(f\"Inputs (Ce que voit le robot) : {env.observation_space.shape}\")\n",
    "print(f\"Outputs (Boutons disponibles) : {env.action_space.n}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "45b23753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing previous runs because reinit is set to 'default'."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[32m\u001b[41mERROR\u001b[0m The nbformat package was not found. It is required to save notebook history.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DQN-Baseline-Local-GPU</strong> at: <a href='https://wandb.ai/ravi-sabra-cpe-lyon/Projet-Trading-RL/runs/ih5lynhx' target=\"_blank\">https://wandb.ai/ravi-sabra-cpe-lyon/Projet-Trading-RL/runs/ih5lynhx</a><br> View project at: <a href='https://wandb.ai/ravi-sabra-cpe-lyon/Projet-Trading-RL' target=\"_blank\">https://wandb.ai/ravi-sabra-cpe-lyon/Projet-Trading-RL</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251212_011628-ih5lynhx\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.23.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>d:\\Ravi\\5A\\Reinforcement Learning\\projet\\Projet_Reinforcement_Learning\\wandb\\run-20251212_011641-n2v4g85b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/ravi-sabra-cpe-lyon/Projet-Trading-RL/runs/n2v4g85b' target=\"_blank\">DQN-Baseline-Local-GPU</a></strong> to <a href='https://wandb.ai/ravi-sabra-cpe-lyon/Projet-Trading-RL' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/ravi-sabra-cpe-lyon/Projet-Trading-RL' target=\"_blank\">https://wandb.ai/ravi-sabra-cpe-lyon/Projet-Trading-RL</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/ravi-sabra-cpe-lyon/Projet-Trading-RL/runs/n2v4g85b' target=\"_blank\">https://wandb.ai/ravi-sabra-cpe-lyon/Projet-Trading-RL/runs/n2v4g85b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " WandB connecté. Les courbes vont apparaître en ligne.\n"
     ]
    }
   ],
   "source": [
    "# On initialise une nouvelle expérience.\n",
    "# \"id\" permet de reprendre une courbe si ça plante, mais ici on laisse vide pour une nouvelle.\n",
    "wandb.init(\n",
    "    project=\"Projet-Trading-RL\",\n",
    "    name=\"DQN-Baseline-Local-GPU\",\n",
    "    config={\n",
    "        \"architecture\": \"Vanilla DQN\",\n",
    "        \"dataset\": \"Multi-Asset (Binance & Yahoo)\",\n",
    "        \"features\": \"Log Returns\",\n",
    "        \"initial_capital\": 1000,\n",
    "        \"gamma\": 0.95,\n",
    "        \"epsilon_start\": 1.0,\n",
    "        \"epsilon_decay\": 0.995,\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 32,\n",
    "        \"episodes\": 20  # Tu pourras augmenter ce chiffre plus tard\n",
    "    }\n",
    ")\n",
    "\n",
    "print(\" WandB connecté. Les courbes vont apparaître en ligne.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec7806c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Démarrage de l'entraînement sur cuda pour 20 épisodes...\n",
      "--------------------------------------------------\n",
      "Market Return : 902.89%   |   Portfolio Return : -100.00%   |   Valuation Finale : 0.020621328804258432   |   \n",
      "Episode 1/20 Terminé.\n",
      "   Score: 0.02$ | ROI: -100.00% | Epsilon: 0.010\n",
      "--------------------------------------------------\n",
      "Market Return : 40.24%   |   Portfolio Return : -56.48%   |   Valuation Finale : 435.18757484630504   |   \n",
      "Episode 2/20 Terminé.\n",
      "   Score: 435.19$ | ROI: -56.48% | Epsilon: 0.010\n",
      "--------------------------------------------------\n",
      "Market Return : 27.57%   |   Portfolio Return : -56.95%   |   Valuation Finale : 430.5303087160685   |   \n",
      "Episode 3/20 Terminé.\n",
      "   Score: 430.53$ | ROI: -56.95% | Epsilon: 0.010\n",
      "--------------------------------------------------\n",
      "Market Return : 209.83%   |   Portfolio Return : -54.73%   |   Valuation Finale : 452.731540647851   |   \n",
      "Episode 4/20 Terminé.\n",
      "   Score: 452.73$ | ROI: -54.73% | Epsilon: 0.010\n",
      "--------------------------------------------------\n",
      "Market Return : 10.31%   |   Portfolio Return : -54.95%   |   Valuation Finale : 450.50281259555544   |   \n",
      "Episode 5/20 Terminé.\n",
      "   Score: 450.50$ | ROI: -54.95% | Epsilon: 0.010\n",
      "--------------------------------------------------\n",
      "Market Return : 102.45%   |   Portfolio Return : -95.63%   |   Valuation Finale : 43.67516255726122   |   \n",
      "Episode 6/20 Terminé.\n",
      "   Score: 43.68$ | ROI: -95.63% | Epsilon: 0.010\n",
      "--------------------------------------------------\n",
      "Market Return :  5.19%   |   Portfolio Return : -97.23%   |   Valuation Finale : 27.65345220879741   |   \n",
      "Episode 7/20 Terminé.\n",
      "   Score: 27.65$ | ROI: -97.23% | Epsilon: 0.010\n",
      "--------------------------------------------------\n",
      "   Episode 8 | Step 27500 | Val: 6$ | Eps: 0.0111\n",
      "Entraînement interrompu manuellement.\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Episode</td><td>▁▂▃▅▆▇█</td></tr><tr><td>Epsilon</td><td>▁▁▁▁▁▁▁</td></tr><tr><td>Market Return</td><td>█▁▁▃▁▂▁</td></tr><tr><td>Portfolio Return</td><td>▁████▂▁</td></tr><tr><td>Valuation</td><td>▁████▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Episode</td><td>7</td></tr><tr><td>Epsilon</td><td>0.01</td></tr><tr><td>Market Return</td><td>5.19</td></tr><tr><td>Portfolio Return</td><td>-97.23</td></tr><tr><td>Valuation</td><td>27.65345</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">DQN-Baseline-Local-GPU</strong> at: <a href='https://wandb.ai/ravi-sabra-cpe-lyon/Projet-Trading-RL/runs/n2v4g85b' target=\"_blank\">https://wandb.ai/ravi-sabra-cpe-lyon/Projet-Trading-RL/runs/n2v4g85b</a><br> View project at: <a href='https://wandb.ai/ravi-sabra-cpe-lyon/Projet-Trading-RL' target=\"_blank\">https://wandb.ai/ravi-sabra-cpe-lyon/Projet-Trading-RL</a><br>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20251212_011641-n2v4g85b\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Session terminée.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Paramètres de la session\n",
    "EPISODES = 20           # Nombre de parties à jouer\n",
    "BATCH_SIZE = 32         # Taille du lot pour l'apprentissage\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "\n",
    "print(f\"Démarrage de l'entraînement sur {device} pour {EPISODES} épisodes...\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "try:\n",
    "    for e in range(1, EPISODES + 1):\n",
    "        state, info = env.reset()\n",
    "        \n",
    "        # Formatage state\n",
    "        if isinstance(state, tuple): state = state[0]\n",
    "        state = np.reshape(state, [1, state_size])\n",
    "        \n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        step_count = 0\n",
    "        \n",
    "        while not done:\n",
    "            # Action\n",
    "            action = agent.act(state)\n",
    "            \n",
    "            # Step\n",
    "            next_state, reward, terminated, truncated, info = env.step(action)\n",
    "            done = terminated or truncated\n",
    "            \n",
    "            # Formatage next_state\n",
    "            next_state = np.reshape(next_state, [1, state_size])\n",
    "            \n",
    "            # Mémorisation\n",
    "            agent.remember(state, action, reward, next_state, done)\n",
    "            \n",
    "            state = next_state\n",
    "            total_reward += reward\n",
    "            step_count += 1\n",
    "            \n",
    "            # Replay\n",
    "            if len(agent.memory) > BATCH_SIZE:\n",
    "                agent.replay(BATCH_SIZE)\n",
    "            \n",
    "            if step_count % 500 == 0:\n",
    "                print(f\"   Episode {e} | Step {step_count} | Val: {info['portfolio_valuation']:.0f}$ | Eps: {agent.epsilon:.2f}\", end='\\r')\n",
    "\n",
    "        # --- Fin de l'épisode ---\n",
    "        \n",
    "        # on utilise .unwrapped pour accéder aux métriques\n",
    "        metrics = env.unwrapped.get_metrics()\n",
    "        final_val = metrics['Valuation Finale']\n",
    "        \n",
    "        print(f\"Episode {e}/{EPISODES} Terminé.\")\n",
    "        # On nettoie les % pour l'affichage (s'ils sont en string)\n",
    "        p_return = metrics['Portfolio Return']\n",
    "        print(f\"   Score: {final_val:.2f}$ | ROI: {p_return} | Epsilon: {agent.epsilon:.3f}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Envoi à WandB\n",
    "        # Petite sécurité supplémentaire pour convertir les strings \"10%\" en float 10.0\n",
    "        try:\n",
    "            market_ret = float(str(metrics['Market Return']).strip('%'))\n",
    "        except: market_ret = 0.0\n",
    "        \n",
    "        try:\n",
    "            port_ret = float(str(metrics['Portfolio Return']).strip('%'))\n",
    "        except: port_ret = 0.0\n",
    "\n",
    "        wandb.log({\n",
    "            \"Episode\": e,\n",
    "            \"Valuation\": final_val,\n",
    "            \"Epsilon\": agent.epsilon,\n",
    "            \"Market Return\": market_ret,\n",
    "            \"Portfolio Return\": port_ret\n",
    "        })\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nEntraînement interrompu manuellement.\")\n",
    "\n",
    "finally:\n",
    "    wandb.finish()\n",
    "    env.close()\n",
    "    print(\"Session terminée.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (Mon GPU2)",
   "language": "python",
   "name": "venv_gpu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
